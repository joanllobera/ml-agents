{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wall Jump ML-Agents Example\n",
    "---\n",
    "## Introduction\n",
    "---\n",
    "This notebook examines the [**WallJump**](https://www.youtube.com/watch?v=NITLug2DIWQ&feature=youtu.be) example of [Unity's ML-Agents](https://github.com/Unity-Technologies/ml-agents) repository. This case is about an agent that must jump through a variable-height wall, sometimes with the help of a box, which must move to jump on it, through the wall and reach the target. \n",
    "\n",
    "Specifically we will explain the operation of the variables and rewards of the `WallJump` scene of Unity. Also will train a new brain adding modifications to the enviroment to later compare the results obtained with the trained brain in an environment without modifications thanks to the [TensorBoard](https://www.tensorflow.org/guide/summaries_and_tensorboard) graphics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Members\n",
    "---\n",
    "### Miquel Ripoll Fornes\n",
    "<img style=\"float: left; margin-right: 20px;\" src='img/miquel.png' width=\"100\" height=\"100\"/> <br/>\n",
    "\n",
    "- __*Location*__: Mallorca   \n",
    "- __*Mail*__: miquelripollfornes@enti.cat \n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "### Marc Martos Cabré\n",
    "<img style=\"float: left; margin-right: 20px;\" src='img/marc.png' width=\"100\" height=\"100\"/> <br/>\n",
    "\n",
    "- __*Location*__: Barcelona\n",
    "- __*Mail*__: marcmartoscabre@enti.cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Analysis\n",
    "---\n",
    "First of all, we take a look at our example in the [Unity's ML-Agents](https://github.com/Unity-Technologies/ml-agents) repository. We have an agent on scene (blue box) which it goal is to reach the marked area on the ground. The scene will have three different configurations.\n",
    "\n",
    "Each configuration challenge the actor in different ways, making him thinking different on every iteration. The configurations, modify the wall height and this will change the actor behaviour. \n",
    "\n",
    "On the `bigWall` configuration, if the agent detects a wall, he will instantly try to find a box to jump over the wall. On the two other cases, he will just move on throught the area and he will jump without any problem over the wall. \n",
    "\n",
    "<img align='center' src='img/WallJumpExample.gif'>\n",
    "\n",
    "### Agent\n",
    "---\n",
    "The agent is a blue cube which have **three brains** on it. This three brains will act in different situations such as the mentioned before, depending on the height of the wall. We will use the same brain for the \"No wall\" case and the \"Small Wall\" case, because the player does not need to interact with the block in order to overcome the wall, he can just jump over it. \n",
    "\n",
    "This case has **four states**, which are: the agent position, the goal position, the wall position and his height and the block position. This states are directly related with the observations.\n",
    "\n",
    "The agent can do **four different actions** on each frame depending on the actual states, he can move on forward or backward, rotate around his up vector, move to sides and he can also jump. This actions are in the `MoveAgent` method, used in `AgentAction` method.\n",
    "\n",
    "<img align='center' src='img/actions.png'>\n",
    "\n",
    "In the first place, to make this actions, the actor needs the information about the other objects of the scene. For this purpose, the agent have the `CollectObservations` method which uses the Raycasts to do this job.\n",
    "\n",
    "Finally, to do the correct and optimal action considering the observations collected we need to train it. To do that, the **rewards** will be used with the `AddReward` method.\n",
    "\n",
    "### Ray Perception\n",
    "---\n",
    "As we said before, the agent needs to **throw raycasts** to collect his observations and put them in a vector. To do that the agent will throw rays in different angles and heights, depending on its position. This **rays will collide against an object** of the scene and this will give all the information the actor needs from the world, as the **position** of the objects or his **height**. We have **three** different objects to collide the ray against in the example: \n",
    "\n",
    "- Wall: The obstacle the actor will face on the area, the raycast could give us the **distance and height** of this.\n",
    "- Goal: The area the actor have to reach, the raycast give us the **distance** between the agent and goal.\n",
    "- Block: The object which will help the player to get through the wall, the raycast give us the **distance and if the agent are on it**.\n",
    "\n",
    "> Note: There is another raycast down that continuously checks if the agent is **grounded or falling**.\n",
    "\n",
    "<img align='center' src='img/Raycasts.png' width=\"200\" height=\"200\"> \n",
    "\n",
    "In this case we have rays in 7 types angles on 2 diferent heights, offering a total of 14 rays. This means for every detectable object we need 14 `vectorObservation` slots on the agent brains. This is usefull for the future implementation of our case.\n",
    "\n",
    "### Rewards\n",
    "---\n",
    "Agents learn by trial and error when receiving a status. So for training to take effect is necessary to give the appropriate **rewards/punishments** at the right time. These rewards are **associated to an action** given a concrete state, reason why with sufficient experience the agent can finish knowing which is the best option of all given a state to maximize its reward.\n",
    "\n",
    "In this example, we have 4 kinds of rewards, 3 setted with `SetReward` method, **overriding** the current step reward of the agent and updates the episode reward accordingly and 1 added with `AddReward` method, **incrementing** the step and episode rewards by the provided value:\n",
    "\n",
    "- Agent falls: `SetReward(-1f)`.\n",
    "- Block falls: `SetReward(-1f)`.\n",
    "- Agent reach goal: `SetReward(1f)`.\n",
    "- Agent move: `AddReward(-0.0005f)` to current reward.\n",
    "\n",
    "The values of the rewards are normalized, for that reason moving an agent adds `-0.0005f`, because the maximum steps of each episode are 2000, which ends up with a total of `-1f` at the end of the episode. If we change one of these two values is mandatory change the other accordly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Analysis\n",
    "---\n",
    "### Parameters\n",
    "\n",
    "There are a lot of parameters in `trainer_config.yaml` which might show different results on your trainings. Before start, we need to know all the brains goes to the config file to found his header and take his parameters, but if they couldn't find it the brain uses the default header parameters. So we will explain few parameters we consider the most relevant for our experiment, make an optimization of the configuration to train the agent more efficiently:\n",
    "\n",
    "<img src='img/config.png'>\n",
    "\n",
    "#### Learning rate:\n",
    "This parameter is **how much value have an action** for the agent while he's training. Therefore the agent will learn slower the lower this parameter is. On the other hand we shouldn't have a very high value, so the agent will not explore enough and will be satisfied with the first actions that reward him in a positive way, which can generate that he doesn't learn the optimal action to perform. <br/>\n",
    "Default value: `0.0003`\n",
    "\n",
    "#### Max steps:\n",
    "As the name implies, this means that the higher this parameter is, the longer the agent will train. We can define it as runs or the **times the environment is restarted** before completing the training. <br/>\n",
    "Default value: `1.000.000`\n",
    "\n",
    "#### Hidden units:\n",
    "Hidden units are the number of units in the hidden layers of the neural network. This parameter give the agent more **complexity and recombination of input factors** so that he can find different solutions. For problems where the action is a very complex interaction between the observation variables, this should be larger. <br/>\n",
    "Default value: `256`\n",
    "\n",
    "#### Normalize:\n",
    "Corresponds to whether normalization is applied to the **vector observation inputs**. This normalization is based on the running average and variance of the vector observation. Normalization can be helpful in cases with **complex continuous control problems**, but may be harmful with simpler discrete control problems. <br/>\n",
    "Default value: `false`\n",
    "\n",
    "### Tweaking parameters:\n",
    "\n",
    "The first thing we did was try to reduce the number of `max_steps` in training, so we tried to see if 500k had enough to train our brains (small & big). We could see that the `smallWallBrain` had **enough with 50k** to find a valid solution and between **150k-200k to optimize** it. While the `bigWallBrain`, at **50k he learns not to die** so his trainings are longer, but **500k not enough** for him to learn which is the best option to achieve the Goal.\n",
    "\n",
    "Having checked this, we decided to check the **proportionality** of the `learningRate` with the Steps, so we doubled the value from `0.0003` to `0.0006`, hoping that the training would be much more effective and we encountered these results:\n",
    "\n",
    "<img align='center' src='img/learningRate.png'>\n",
    "\n",
    "As we can see the results did **not change** at all, showing us that the learningRate of this example is already sufficiently optimized and that therefore it did not help to increase it.\n",
    "\n",
    "The next step was changing the `normalize` value from `false` to `true`. Because, as we said before, the normalization is helpful in cases with **complex continuous control problems** as our example, at least the big wall case. We really didn’t expect much of this change, but...\n",
    "\n",
    "<img align='center' src='img/normalize.png'>\n",
    "\n",
    "On the `smallWallBrain` side showed no change as we expect, but on the `bigWallBrain` we had a big surprise. The normalization makes it possible for the agent to find a **valid solution at 300k-350k**, and needs more or less **500k to optimize** it. This was a great help in achieving the Steps optimization, we were looking for.\n",
    "\n",
    "Last step in our experiment to optimize the training was change the `hidden_units` and try to use a larger value, as it is better for complex problems. We changed the value to `512` and we obtained this: \n",
    "\n",
    "<img align='center' src='img/normalize_hu.png'>\n",
    "\n",
    "In this case, we had the opposite result as we expect. Increase the `hiden_units` till 512 not only has the training not improved, it has also worsened its efficiency. So it seems that the case was not complicated enough for that value, so the initial value is better adapted to the example.\n",
    "\n",
    "Finally, we have our optimized `trainer_config.yaml` file, obtaining a significant reduction in the steps necessary for the most difficult case (_*big wall*_).\n",
    "\n",
    "| Parameter     | Default Config  | Our Optimized Config |\n",
    "| ---           |       ---       |         ---          |\n",
    "| Learning Rate | `0.0003`        | `0.0003`             |\n",
    "| Max Steps     | `1.000.000`     | `500.000`            |\n",
    "| Hidden Units  | `256`           | `256`                |\n",
    "| Normalize     | `false`         | `true`               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Case Proposal\n",
    "---\n",
    "For our own case we have decided to **add an enemy** to the scene so that the agent has to dodge it and make it difficult to get to goal, because if the agent touches him dies and re-init the scene. The initial hypothesis is that the agent learns to dodge the enemies while learning to jump the wall with the help of the block. \n",
    "\n",
    "In this case the agent can perform the **same actions** as in the example: move, rotate and jump. On the other hand, the **states have increased** because there is now an enemy on the scene.\n",
    "\n",
    "For this example we gonna copy the `WallJumpAgent.cs`, rename to `WallJumpAgentUpgrade.cs` and do all the changes on it. \n",
    "\n",
    "### Enemy\n",
    "We have decided to add an enemy which can not be jump over by the agent and which moves in a straight line on the x-axis. The enemy can be modified the speed and size to regulate the difficulty. To implement it only needs to create a `Cube` in the `WallJumpArea` prefab, put in front of the wall, set a coherent size _*(we used x=3, y=1.5, z=1)*_ and drag the `EnemyBehavior.cs` we made it. Could also be changed the speed of the agent in the inspector.\n",
    "\n",
    "<img align='center' src='img/enemy.png' width=\"300\">\n",
    "\n",
    "\n",
    "### Observations\n",
    "As we said before, we have added a new element in the scene, it means **more states and more observations**. We had to add more observations to the agent with the help of the RayPerception. For this reason, as we explained on the Case Analysis section, need to increase the `vectorObservations` of the **two brains** _*(Big & Small)*_ in 14, from 74 to 88.\n",
    "\n",
    "<img align='center' src='img/brain.png' width=\"300\">\n",
    "\n",
    "Once that is done, we need to say the agent what is an enemy. To do that, first of all need to go one more time to the `Enemy` and **tag it** as a `stone` in the inspector (create the new tag or a similar one).\n",
    "\n",
    "<img align='center' src='img/tag.png' width=\"300\">\n",
    "\n",
    "Now it's necessary to declare on the `WallBallAgentUpgrade.cs`, putting the new tag on the `detectableObjects` array.\n",
    "\n",
    "<img align='center' src='img/detectable.png' width=\"400\">\n",
    "\n",
    "### Rewards\n",
    "To show the agent that the enemy is an obstacle to his target, we decided to give him a **negative reward** every time he came into contact with him. This Reward had to be forceful enough to dodge it, so we typed the same value as the fall of the agent or the block _*(SetReward(-1f)*_. Thus, when the agent touches the enemy, the step is restarted and given a negative reward.\n",
    "\n",
    "<img align='center' src='img/collision.png' width=\"400\">\n",
    "\n",
    "With this first version we did not have enough, since the agent learned not to die and to dodge the enemy, but never learned to pass the wall. To try to solve this, we try to add one more reward to the setup, this time we would add a reward the first time the agent brought the block closer to the wall at a predetermined distance. We were varying this reward between 0.2f and 0.5f to see that it really didn’t help the final solution we were looking for. Then we end up discarding it.\n",
    "\n",
    "<img align='center' src='img/help.png'>\n",
    "\n",
    "Finally, with some testing we found the correct way to train the agent. We only need to put the enemy in some cases, not all cases. To do that we decide to exclude the enemy in the 50% of the Big Wall cases, changing his 'y' position when the enemy resets.\n",
    "\n",
    "<img align='center' src='img/reset.png' width=\"450\">\n",
    "\n",
    "It was our final and optimal solution for this example and we can see the results of it in the tensorboard graphics.\n",
    "\n",
    "<img align='center' src='img/finalBoard.png'>\n",
    "\n",
    "<img align='center' src='img/finalGif.gif'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annex\n",
    "\n",
    "### EnemyBehavior.cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using System.Collections;\n",
    "using System.Collections.Generic;\n",
    "using UnityEngine;\n",
    "\n",
    "public class EnemyBehavior : MonoBehaviour{\n",
    "    private bool dirRight = true;\n",
    "    public float len = 9.0f;\n",
    "    public float speed = 2.0f;\n",
    "\n",
    "    // Update is called once per frame\n",
    "    void Update() {\n",
    "        if (dirRight)\n",
    "            transform.Translate(Vector2.right * speed * Time.deltaTime);\n",
    "        else\n",
    "            transform.Translate(-Vector2.right * speed * Time.deltaTime);\n",
    "\n",
    "        if (transform.position.x >= len) dirRight = false;\n",
    "        if (transform.position.x <= -len) dirRight = true;\n",
    "    }\n",
    "\n",
    "    void OnCollisionEnter(Collision col) {\n",
    "        dirRight = !dirRight;\n",
    "    }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WallBallAgentUpgrade.cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using System.Collections;\n",
    "using System.Collections.Generic;\n",
    "using UnityEngine;\n",
    "using System.Linq;\n",
    "using MLAgents;\n",
    "\n",
    "public class WallJumpAgentUpgrade : Agent\n",
    "{\n",
    "    // Depending on this value, the wall will have different height\n",
    "    int configuration;\n",
    "    // Brain to use when no wall is present\n",
    "    public Brain noWallBrain;\n",
    "    // Brain to use when a jumpable wall is present\n",
    "    public Brain smallWallBrain;\n",
    "    // Brain to use when a wall requiring a block to jump over is present\n",
    "    public Brain bigWallBrain;\n",
    "\n",
    "    public GameObject ground;\n",
    "    public GameObject spawnArea;\n",
    "    Bounds spawnAreaBounds;\n",
    "\n",
    "\n",
    "    public GameObject goal;\n",
    "    public GameObject shortBlock;\n",
    "    public GameObject enemy;\n",
    "    public GameObject wall;\n",
    "    Rigidbody shortBlockRB;\n",
    "    Rigidbody agentRB;\n",
    "    Material groundMaterial;\n",
    "    Renderer groundRenderer;\n",
    "    WallJumpAcademy academy;\n",
    "    RayPerception rayPer;\n",
    "\n",
    "    public bool together = false;\n",
    "    public float jumpingTime;\n",
    "    public float jumpTime;\n",
    "    // This is a downward force applied when falling to make jumps look\n",
    "    // less floaty\n",
    "    public float fallingForce;\n",
    "    // Use to check the coliding objects\n",
    "    public Collider[] hitGroundColliders = new Collider[3];\n",
    "    Vector3 jumpTargetPos;\n",
    "    Vector3 jumpStartingPos;\n",
    "\n",
    "    string[] detectableObjects;\n",
    "\n",
    "    public override void InitializeAgent()\n",
    "    {\n",
    "        academy = FindObjectOfType<WallJumpAcademy>();\n",
    "        rayPer = GetComponent<RayPerception>();\n",
    "        configuration = Random.Range(0, 5);\n",
    "        detectableObjects = new string[] { \"wall\", \"goal\", \"block\", \"stone\" };\n",
    "\n",
    "        agentRB = GetComponent<Rigidbody>();\n",
    "        shortBlockRB = shortBlock.GetComponent<Rigidbody>();\n",
    "        spawnAreaBounds = spawnArea.GetComponent<Collider>().bounds;\n",
    "        groundRenderer = ground.GetComponent<Renderer>();\n",
    "        groundMaterial = groundRenderer.material;\n",
    "\n",
    "        spawnArea.SetActive(false);\n",
    "    }\n",
    "\n",
    "\n",
    "    // Begin the jump sequence\n",
    "    public void Jump()\n",
    "    {\n",
    "        jumpingTime = 0.2f;\n",
    "        jumpStartingPos = agentRB.position;\n",
    "    }\n",
    "\n",
    "    /// <summary>\n",
    "    /// Does the ground check.\n",
    "    /// </summary>\n",
    "    /// <returns><c>true</c>, if the agent is on the ground, \n",
    "    /// <c>false</c> otherwise.</returns>\n",
    "    /// <param name=\"boxWidth\">The width of the box used to perform \n",
    "    /// the ground check. </param>\n",
    "    public bool DoGroundCheck(bool smallCheck)\n",
    "    {\n",
    "        if (!smallCheck)\n",
    "        {\n",
    "            hitGroundColliders = new Collider[3];\n",
    "            Physics.OverlapBoxNonAlloc(\n",
    "                gameObject.transform.position + new Vector3(0, -0.05f, 0),\n",
    "                new Vector3(0.95f / 2f, 0.5f, 0.95f / 2f),\n",
    "                hitGroundColliders,\n",
    "                gameObject.transform.rotation);\n",
    "            bool grounded = false;\n",
    "            foreach (Collider col in hitGroundColliders)\n",
    "            {\n",
    "\n",
    "                if (col != null && col.transform != this.transform &&\n",
    "                    (col.CompareTag(\"walkableSurface\") ||\n",
    "                     col.CompareTag(\"block\") ||\n",
    "                     col.CompareTag(\"wall\")))\n",
    "                {\n",
    "                    grounded = true; //then we're grounded\n",
    "                    break;\n",
    "                }\n",
    "            }\n",
    "            return grounded;\n",
    "        }\n",
    "        else\n",
    "        {\n",
    "\n",
    "            RaycastHit hit;\n",
    "            Physics.Raycast(transform.position + new Vector3(0, -0.05f, 0), -Vector3.up, out hit,\n",
    "                1f);\n",
    "\n",
    "            if (hit.collider != null &&\n",
    "                (hit.collider.CompareTag(\"walkableSurface\") ||\n",
    "                 hit.collider.CompareTag(\"block\") ||\n",
    "                 hit.collider.CompareTag(\"wall\"))\n",
    "                && hit.normal.y > 0.95f)\n",
    "            {\n",
    "                return true;\n",
    "            }\n",
    "\n",
    "            return false;\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "    /// <summary>\n",
    "    /// Moves  a rigidbody towards a position smoothly.\n",
    "    /// </summary>\n",
    "    /// <param name=\"targetPos\">Target position.</param>\n",
    "    /// <param name=\"rb\">The rigidbody to be moved.</param>\n",
    "    /// <param name=\"targetVel\">The velocity to target during the\n",
    "    ///  motion.</param>\n",
    "    /// <param name=\"maxVel\">The maximum velocity posible.</param>\n",
    "    void MoveTowards(Vector3 targetPos, Rigidbody rb, float targetVel, float maxVel) {\n",
    "        Vector3 moveToPos = targetPos - rb.worldCenterOfMass;\n",
    "        Vector3 velocityTarget = moveToPos * targetVel * Time.fixedDeltaTime;\n",
    "        if (float.IsNaN(velocityTarget.x) == false) {\n",
    "            rb.velocity = Vector3.MoveTowards(\n",
    "                rb.velocity, velocityTarget, maxVel);\n",
    "        }\n",
    "    }\n",
    "\n",
    "    public override void CollectObservations() {\n",
    "            float rayDistance = 20f;\n",
    "            float[] rayAngles = { 0f, 45f, 90f, 135f, 180f, 110f, 70f };\n",
    "            AddVectorObs(rayPer.Perceive(\n",
    "                rayDistance, rayAngles, detectableObjects, 0f, 0f));\n",
    "            AddVectorObs(rayPer.Perceive(\n",
    "                rayDistance, rayAngles, detectableObjects, 2.5f, 2.5f));\n",
    "            Vector3 agentPos = agentRB.position - ground.transform.position;\n",
    "\n",
    "            AddVectorObs(agentPos / 20f);\n",
    "            AddVectorObs(DoGroundCheck(true) ? 1 : 0);\n",
    "    }\n",
    "\n",
    "    /// <summary>\n",
    "    /// Gets a random spawn position in the spawningArea.\n",
    "    /// </summary>\n",
    "    /// <returns>The random spawn position.</returns>\n",
    "    public Vector3 GetRandomSpawnPos() {\n",
    "        Vector3 randomSpawnPos = Vector3.zero;\n",
    "        float randomPosX = Random.Range(-spawnAreaBounds.extents.x,\n",
    "                                        spawnAreaBounds.extents.x);\n",
    "        float randomPosZ = Random.Range(-spawnAreaBounds.extents.z,\n",
    "                                        spawnAreaBounds.extents.z);\n",
    "\n",
    "        randomSpawnPos = spawnArea.transform.position +\n",
    "                                  new Vector3(randomPosX, 0.45f, randomPosZ);\n",
    "        return randomSpawnPos;\n",
    "    }\n",
    "\n",
    "    /// <summary>\n",
    "    /// Chenges the color of the ground for a moment\n",
    "    /// </summary>\n",
    "    /// <returns>The Enumerator to be used in a Coroutine</returns>\n",
    "    /// <param name=\"mat\">The material to be swaped.</param>\n",
    "    /// <param name=\"time\">The time the material will remain.</param>\n",
    "    IEnumerator GoalScoredSwapGroundMaterial(Material mat, float time) {\n",
    "        groundRenderer.material = mat;\n",
    "        yield return new WaitForSeconds(time); //wait for 2 sec\n",
    "        groundRenderer.material = groundMaterial;\n",
    "    }\n",
    "\n",
    "\n",
    "    public void MoveAgent(float[] act) {\n",
    "        AddReward(-0.0005f);\n",
    "        bool smallGrounded = DoGroundCheck(true);\n",
    "        bool largeGrounded = DoGroundCheck(false);\n",
    "\n",
    "        Vector3 dirToGo = Vector3.zero;\n",
    "        Vector3 rotateDir = Vector3.zero;\n",
    "        int dirToGoForwardAction = (int) act[0];\n",
    "        int rotateDirAction = (int) act[1];\n",
    "        int dirToGoSideAction = (int) act[2];\n",
    "        int jumpAction = (int) act[3];\n",
    "\n",
    "        if (dirToGoForwardAction==1)\n",
    "            dirToGo = transform.forward * 1f * (largeGrounded ? 1f : 0.5f);\n",
    "        else if (dirToGoForwardAction==2)\n",
    "            dirToGo = transform.forward * -1f * (largeGrounded ? 1f : 0.5f);\n",
    "        if (rotateDirAction==1)\n",
    "            rotateDir = transform.up * -1f;\n",
    "        else if (rotateDirAction==2)\n",
    "            rotateDir = transform.up * 1f;\n",
    "        if (dirToGoSideAction==1)\n",
    "            dirToGo = transform.right * -0.6f * (largeGrounded ? 1f : 0.5f);\n",
    "        else if (dirToGoSideAction==2)\n",
    "            dirToGo = transform.right * 0.6f * (largeGrounded ? 1f : 0.5f);\n",
    "        if (jumpAction == 1)\n",
    "            if ((jumpingTime <= 0f) && smallGrounded) {\n",
    "                Jump();\n",
    "            }\n",
    "\n",
    "        transform.Rotate(rotateDir, Time.fixedDeltaTime * 300f);\n",
    "        agentRB.AddForce(dirToGo * academy.agentRunSpeed,\n",
    "                         ForceMode.VelocityChange);\n",
    "\n",
    "        if (jumpingTime > 0f) {\n",
    "            jumpTargetPos =\n",
    "            new Vector3(agentRB.position.x,\n",
    "                        jumpStartingPos.y + academy.agentJumpHeight,\n",
    "                        agentRB.position.z) + dirToGo;\n",
    "            MoveTowards(jumpTargetPos, agentRB, academy.agentJumpVelocity,\n",
    "                        academy.agentJumpVelocityMaxChange);\n",
    "\n",
    "        }\n",
    "\n",
    "        if (!(jumpingTime > 0f) && !largeGrounded) {\n",
    "            agentRB.AddForce(\n",
    "            Vector3.down * fallingForce, ForceMode.Acceleration);\n",
    "        }\n",
    "        jumpingTime -= Time.fixedDeltaTime;\n",
    "    }\n",
    "\n",
    "    public override void AgentAction(float[] vectorAction, string textAction) {\n",
    "        MoveAgent(vectorAction);\n",
    "        if ((!Physics.Raycast(agentRB.position, Vector3.down, 20))\n",
    "            || (!Physics.Raycast(shortBlockRB.position, Vector3.down, 20))) {\n",
    "            Done();\n",
    "            SetReward(-1f);\n",
    "            ResetBlock(shortBlockRB);\n",
    "            ResetEnemy(enemy);\n",
    "            StartCoroutine(\n",
    "                GoalScoredSwapGroundMaterial(academy.failMaterial, .5f));\n",
    "        }\n",
    "        //if (Mathf.Abs(shortBlock.transform.position.z - wall.transform.position.z) <= 3.0\n",
    "        //    && !together) { // help the player use the box\n",
    "        //    AddReward(0.5f);\n",
    "        //    together = true;\n",
    "        //}\n",
    "    }\n",
    "\n",
    "    // Detect when the agent hits the enemy\n",
    "    void OnCollisionEnter(Collision col) {\n",
    "        if (col.gameObject.CompareTag(\"stone\")) {\n",
    "            Done();\n",
    "            SetReward(-1f);\n",
    "            ResetBlock(shortBlockRB);\n",
    "            ResetEnemy(enemy);\n",
    "            StartCoroutine(\n",
    "                GoalScoredSwapGroundMaterial(academy.failMaterial, .5f));\n",
    "            Debug.Log(\"I have killed the player\");\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Detect when the agent hits the goal\n",
    "    void OnTriggerStay(Collider col) {\n",
    "        if (col.gameObject.CompareTag(\"goal\") && DoGroundCheck(true)) {\n",
    "            SetReward(1f);\n",
    "            Done();\n",
    "            StartCoroutine(\n",
    "                GoalScoredSwapGroundMaterial(academy.goalScoredMaterial, 2));\n",
    "        }\n",
    "    }\n",
    "\n",
    "    //Reset the orange block position\n",
    "    void ResetBlock(Rigidbody blockRB) {\n",
    "        blockRB.transform.position = GetRandomSpawnPos();\n",
    "        blockRB.velocity = Vector3.zero;\n",
    "        blockRB.angularVelocity = Vector3.zero;\n",
    "    }\n",
    "    \n",
    "    //Reset the enemy position\n",
    "    void ResetEnemy(GameObject enemy) {\n",
    "        if (configuration == 2 || configuration == 3) // CHANGE IT from 5 to 6\n",
    "            enemy.transform.localPosition = new Vector3(0.0f, -100.0f, -5.0f);\n",
    "        else\n",
    "            enemy.transform.localPosition = new Vector3(0.0f, 1.5f, -5.0f);\n",
    "    }\n",
    "\n",
    "    public override void AgentReset() {\n",
    "        ResetBlock(shortBlockRB);\n",
    "        transform.localPosition = new Vector3(\n",
    "            18 * (Random.value - 0.5f), 1, -12);\n",
    "        configuration = Random.Range(0, 6); // CHANGE IT from 5 to 6\n",
    "        ResetEnemy(enemy); // Reset Enemy\n",
    "        agentRB.velocity = default(Vector3);\n",
    "        //together = false;\n",
    "    }\n",
    "\n",
    "    private void FixedUpdate() {\n",
    "        if (configuration != -1) {\n",
    "            ConfigureAgent(configuration);\n",
    "            configuration = -1;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    /// <summary>\n",
    "    /// Configures the agent. Given an integer config, the wall will have\n",
    "    /// different height and a different brain will be assigned to the agent.\n",
    "    /// </summary>\n",
    "    /// <param name=\"config\">Config. \n",
    "    /// If 0 : No wall and noWallBrain.\n",
    "    /// If 1:  Small wall and smallWallBrain.\n",
    "    /// Other : Tall wall and BigWallBrain. </param>\n",
    "    void ConfigureAgent(int config) {\n",
    "        if (config == 0) {\n",
    "            wall.transform.localScale = new Vector3(\n",
    "                wall.transform.localScale.x,\n",
    "                academy.resetParameters[\"no_wall_height\"],\n",
    "                wall.transform.localScale.z);\n",
    "            GiveBrain(noWallBrain);\n",
    "        }\n",
    "        else if (config == 1) {\n",
    "            wall.transform.localScale = new Vector3(\n",
    "                wall.transform.localScale.x,\n",
    "                academy.resetParameters[\"small_wall_height\"],\n",
    "                wall.transform.localScale.z);\n",
    "            GiveBrain(smallWallBrain);\n",
    "        }\n",
    "        else {\n",
    "            float height =\n",
    "                academy.resetParameters[\"big_wall_min_height\"] +\n",
    "                Random.value * (academy.resetParameters[\"big_wall_max_height\"] -\n",
    "                academy.resetParameters[\"big_wall_min_height\"]);\n",
    "            wall.transform.localScale = new Vector3(\n",
    "                wall.transform.localScale.x,\n",
    "                height,\n",
    "                wall.transform.localScale.z);\n",
    "            GiveBrain(bigWallBrain);\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
