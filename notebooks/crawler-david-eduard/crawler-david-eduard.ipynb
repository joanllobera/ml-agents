{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The aim of this notebook is to study the <b>crawler</b> example of the [ml-agents](https://github.com/Unity-Technologies/ml-agents) repository. The crawler is an agent that has a main body and 4 legs composed by 2 limbs each and moves in a plane to reach a target.\n",
    "\n",
    "We will explain how the example works and train one with a modification to compare the results with an unmodified environment using the information that Tensor Board provides us. All of the work done here will be focused on the `CrawlerDynamicLearning` scene from the Unity project.\n",
    "## Team Information\n",
    "### David Pérez Gallego\n",
    "    Student at ENTI-UB: Interactive digital content\n",
    "    Email: davidperezgallego@enti.cat  \n",
    "<img width='150px' align='center' src='img/david.jpg'>\n",
    "\n",
    "### Eduard Arnau Romeu\n",
    "    Student at ENTI-UB: Interactive digital content\n",
    "    Email: eduardarnauromeu@enti.cat    \n",
    "<img width='150px' align='center' src='img/eduard.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case analysis\n",
    "First of all, we will take a look at the crawler as a GameObject and all the parts and scripts involved in it:\n",
    "\n",
    "<img width='900px' align='center' src='img/crawler-gameobject.png'>\n",
    "\n",
    "<center><cite>Crawler GameObject in the ml-agents unity project</cite></center>\n",
    "\n",
    "As we can see, the crawler is formed by <b>4 legs</b> (each one with its relative foreleg) and a <b>body</b>.\n",
    "\n",
    "### Legs\n",
    "Each leg part is attached to its father limb by a configurable joint which applies an angular constraint in each axis and prevents any position variation between the leg part and the anchored component.\n",
    "\n",
    "The upper leg part can rotate in the X and Y axis while the lower part can only rotate in the X axis.\n",
    "\n",
    "The foreleg is attached to the leg which is attached to the main body.\n",
    "\n",
    "#### <center> Main Body ←(Joint)← Leg ←(Joint)← Foreleg </center>\n",
    "\n",
    "Apart from the movement, each leg also has a script called `GroundContact.cs` which checks the collision with the `ground` layer. This script allows us to use the collision with the ground to either punish the agent, set the agent as done or use the collision flag as an observation for the agent.\n",
    "\n",
    "The <b>forelegs are used as observations</b> when they collide with the ground and <b>the upper part of the legs are used for punishment</b> since we don't want the agent to use the upper part of the legs to move to the target.\n",
    "\n",
    "### Body\n",
    "The body also has the `GroundContact.cs` code attached. To prevent the agent from dragging its body while walking, <b>the agent is punished whenever the body collides with the ground.</b>\n",
    "\n",
    "### Controller\n",
    "The agent, which runs in a script called `CrawlerAgent.cs` handles all the behaviour while the important body parts of the body are handled by the script `JointDriveController.cs` to store relevant information for acting and learning of each relevant body part.\n",
    "\n",
    "This last script allows the agent to reset the joints, set their target rotation and their strength in order to achieve the desired behaviour. Each joint has the\n",
    "\n",
    "The controller overrides certain functions from the `Agent.cs` class. The added funcitonalities of those functions are the following:\n",
    "\n",
    "##### InitializeAgent()\n",
    "This function <b>initializes all the agent body parts</b>.\n",
    "\n",
    "First of all stores a reference to the `JointDriveController.cs` script that the agent has attached and then initializes the body parts of the agent. <b>Each body part is stored in a dictionary</b> with the transform of each body part as the key and a custom class `BodyPart` as the data. \n",
    "`BodyPart` is a class which belongs to `JointDriveController.cs` and contains all the relevant information of the body part and allows easy access and modification to the `ConfigurableJoint` attached to the GameObject.\n",
    "\n",
    "##### CollectObservations()\n",
    "This functions <b>collects observations for the agent brain so it can learn</b>.\n",
    "\n",
    "It starts by observing the current position relative to the target (referenced as dirToTarget in the code) andhen it stores the body orientation (up and front) and also its Y position. Remember that the agent is punished whenever the body touches the ground.\n",
    "\n",
    "Now the funcitons proceeds to analyze each body part. For each joint the crawler agent checks if the joint is touching the ground using the code `GroundContact.cs` previously explained, the velocity and the angular velocity of the part. When the joint it is observing is not the body (which is the root joint of the agent), it also stores the position of the joint relative to the body, the current rotation (in each axis) and how much strength the joint is applying with a value between 0 and 1 relative to the maximum force that the joint can apply.\n",
    "\n",
    "##### AgentAction()\n",
    "This function <b>checks if the agent has reached the target</b>, it also <b>updates the joints based on the decision frequency and input action </b> and finally it <b>rewards or punishes the agent.</b>\n",
    "\n",
    "This function starts by checking if any body part has touched the target. In case any is touching it, the agent gets a substantial reward and the target is set to a new random position in the environment.\n",
    "\n",
    "After checking if the target is reached, the direction to the target (referenced as dirToTarget in the code) is updated.\n",
    "\n",
    "\n",
    "#### <center>dirToTarget = target.position - body.position<center>\n",
    "\n",
    "The agent checks if it has to take a decision in the current step and if the flag is set to true it takes action.\n",
    "The agent proceeeds to apply torque in two axis to the upper limbs (X and Y as stated in the Legs part above) and in one axis for the lower limbs (the X axis). After applying torque it sets the joints strength for this decision step.\n",
    "\n",
    "The function now proceeds to reward or punish the agent depending on 3 factors:\n",
    "    - The agent moving towards the objective.\n",
    "    - The agent body facing the target.\n",
    "    - The time taken by the agent to reach the target.\n",
    "\n",
    "Last but not the least, the function increments the decision timer which modifies the decision flag and allows the agent to know if it will have to decide in the next step.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Analysis:\n",
    "### Critical parameters\n",
    "#### Gamma:\n",
    "The gamma parameter is the discount factor for future rewards (how much does the agent care about the future rewards).\n",
    "In this particular problem this factor should be set as high as possible, since our task requires high preparation to get to the target.\n",
    "\n",
    "By setting this value low we get an agent that focuses a lot on staying away from the ground (jumps around really high compared to other trained models) and being well oriented towards the target, but that doesn’t really do a good job at actually getting to that target.\n",
    "<img width='900px' align='center' src='img/CrawlerLowGammaJump.gif'>\n",
    "<center><cite>Gamma = 0.8</cite></center>\n",
    "\n",
    "Also, since the task the agent focuses on is easier than going to the target it stops learning much faster than a model with a high gamma.\n",
    "\n",
    "<img width='650px' align='center' src='img/LowGammaReward.png'>\n",
    "<center><cite>The learning process peaks at 350.000 steps and then stays almost the same</cite></center>\n",
    "\n",
    "As a side note, we did saw something unique to this model, its ability to walk even on its back. This behavior probably appears only on this model thanks to his jumps away from the ground (that often flips him over) and its focus on looking at the target. Since other models don’t jump that high we assume they don’t get the chance to learn how to stay away of the ground on that position.\n",
    "\n",
    "##### Max Steps:\n",
    "This parameter defines the maximum amount of simulation steps that are run on a training session.\n",
    "Since our problem is quite complex in both actions and observations we need a lot of steps for the agent to start becoming efficient at its task.\n",
    "\n",
    "On our tests models need at least 300.000 steps to get good results. With all the other parameters set to default we saw that at around 400.000 the agent stops getting better.\n",
    "\n",
    "<img width='650px' align='center' src='img/CrawlerDefaultShort.gif'>\n",
    "<center><cite>100.000 steps model</cite></center>\n",
    "<img width='650px' align='center' src='img/CrawlerDefaultMedium.gif'>\n",
    "<center><cite>250.000 steps model</cite></center>\n",
    "<img width='650px' align='center' src='img/CrawlerDefaultHigh.gif'>\n",
    "<center><cite>One million steps model</cite></center>\n",
    "\n",
    "<img width='750px' align='center' src='img/CumulativeReward.png'>\n",
    "<center><cite>Reward graphic of the default runs</cite></center>\n",
    "\n",
    "\n",
    "By tweaking some of the parameters (mainly the rate of change of the policy), we trained a model that learned above the default parameters and at 500.000 steps was still getting better.\n",
    "<img width='650px' align='center' src='img/CrawlerEpsilonHigh.gif'>\n",
    "<center><cite>510.000 setps with modified epsilon</cite></center>\n",
    "<img width='750px' align='center' src='img/RewardHighEpsilon.png'>\n",
    "\n",
    "#### Other important settings:\n",
    "Due to the size and complexity of this problem there are some other really important parameters related to the amount of observations and actions:\n",
    "###### Batch size and buffer size:\n",
    "Amount of experiences experimented before doing a gradient decent and learning step (respectively). Having as many observations and continuous actions we need to set both of them as high as possible, else the training wouldn’t be effective since the agent wouldn’t experiment enough.\n",
    "##### Number of layers and hidden units: \n",
    "This two parameters correspond to the type of neural network that the model works on. Since our actions depend on a lot of observations that are related on a complex way we need to set this two parameters to high values (3 and 512 which are the highest recommended by the ml-agents documentation).\n",
    "\n",
    "### Smooth Results:\n",
    "#### Epsilon:\n",
    "This value, corresponds to the amount of change of the policies between each gradient descent. This means that the lower this parameter is the higher the stability of the training (it doesn’t get peaks and drops of the rewards due to changing the policy too much).\n",
    "The problem is that the lower this value is the slower the training becomes, since at the start it takes quite a long time to start getting good results.\n",
    "Still, we think that in a really long run (over 600.000 setps) this might become a good option since the stability does really increase a lot.\n",
    "\n",
    "<img width='900px' align='center' src='img/CrawlerStableRun.gif'>\n",
    "<center><cite>Agent trained with low gamma and high Batch and buffer sizes</cite></center>\n",
    "\n",
    "<img width='900px' align='center' src='img/RewardLowEpsilon.png'>\n",
    "<center><cite>Grey: Defaut     Orange: Epislon 0.1 - Batch 4048  - Buffer 40480 </cite></center>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Train fast\n",
    "Since this problem is so complex there isn't a really fast way to train it without a lot of steps. \n",
    "\n",
    "The best way we found to make the training faster was by increasing the epsilon value, thanks to its big changes in the policies at the start, the agent got better really quickly, but as the training advanced the reward values started to become more and more noisy, slowing the training process at those late stages.\n",
    "<img width='900px' align='center' src='img/RewardHighEpsilon.png'>\n",
    "<center><cite>Grey: Defaut     Blue: Epislon 0.3</cite></center>\n",
    "\n",
    "More investigation should be put into this line, also tweaking the beta value (to try and get more entropy so that the agent keeps trying new policies) and maybe augmenting the batch size and buffer size to try help stabilize the training. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Case Porposal\n",
    "\n",
    "### The walker\n",
    "\n",
    "As a new case porposal we've decided that we are going to <b>turn the crawler into a walker</b> by removing two legs from the main body and making it stand vertically. <b>We did two main iterations</b>. The first one helped us define the custom rewards and punishments that we had to include to achieve the desired behaviour while the second one served as pure testing of the new behaviour after a long training period.\n",
    "\n",
    "<img width='900px' align='center' src='img/modifyed-crawler-0.png'>\n",
    "<center><cite>New crawler layout</cite></center>\n",
    "\n",
    "As we can see, it looks as weird as it can get. The constraints on the legs joints have also been modifyed to fit the needs of the new layout that the agent now has.\n",
    "\n",
    "<img width='250px' align='center' src='img/modifyed-crawler-1.png'>\n",
    "<img width='250px' align='center' src='img/modifyed-crawler-2.png'>\n",
    "<center><cite>The joints constraints now resemble human articulations</cite></center>\n",
    "\n",
    "When all those modifications were done, we had to adapt the current agent code to fit the new action and observation vector.\n",
    "\n",
    "### Behaviour\n",
    "\n",
    "A new script called `CrawlerAgentModifyed.cs` contains the new class that drives the agent. This class is a modification of the\n",
    "`CrawlerAgent.cs` that adapts the observation vector and the action vector and includes 2 new rewards/punishments:\n",
    "    - The body position is not greater or smaller than the original position + a little margin.\n",
    "    - The agent body has the up direction aligned with the up of the world.\n",
    "    \n",
    "The original class has an observation vector size of 129 values. Each leg part took 14 values, the main body takes 7 values to observe and other variables like orientation and position take 10 values, thus making a total of:\n",
    "\n",
    "#### <center> 14 values/legPart * 8 legParts + 7 values * body / 1 body + 10 values = 129 values </center>\n",
    "\n",
    "by removing 2 full legs, we ended up removing 4 leg parts (upper and lower ones) leaving us with:\n",
    "\n",
    "#### <center> 14 values/legPart * 4 legParts + 7 values * body / 1 body + 10 values = 73 values </center>\n",
    "\n",
    "We did not add any observation since the ones that were already present after the removal were more than enough.\n",
    "\n",
    "One last thing that we must do is adapt all the actions that the agent can do. In the original brain, the agent recieves an action vector with a length of 20 as the action input. <b>Each upper leg part takes 3 actions</b> (X and Y desired rotation + joint strength) while <b>the lower leg takes 2 actions</b> (X desired rotation + joint strength). Because we are removing 2 full legs which took 10 values from the action vector in total, <b>the action space is reduced from 20 to 10 values</b>.\n",
    "\n",
    "\n",
    "When all of those modifications are done, <b>we must set the new brain parameters</b> in order to make it work with the new code.\n",
    "\n",
    "### Brain\n",
    "\n",
    "As for the brain, the only modifications relative to the original one are the observation vector and the action vector.\n",
    "\n",
    "<img width='500px' align='center' src='img/modifyed-crawler-brain.png'>\n",
    "<center><cite>ModifyedCrawlerDynamicLearning learning brain parameters</cite></center>\n",
    "\n",
    "### First Training\n",
    "\n",
    "On the first training, we left the crawler training with any modification on the reward system (as stated before). The training was set to end at the <b>1 million iterations</b>. The siumlation was run using the <b>configuration file of the dynamic crawler learning</b>. The agent did reset each time the crawler's head touched the ground but the target did not. We ended up with a kangoroo-wannabe crawler that jumped really well but didn't quite hit the landings.\n",
    "\n",
    "<img width='900px' align='center' src='img/first-training.gif'>\n",
    "<center><cite>Crawler derping around</cite></center>\n",
    "\n",
    "We can now analyze the training using tensorboard to see how did the agent perform.\n",
    "\n",
    "<img width='700px' align='center' src='img/first-training-graph-0.png'>\n",
    "<img width='700px' align='center' src='img/first-training-graph-1.png'>\n",
    "\n",
    "First of all, those are the modifyed parameters of the configuration yaml file:\n",
    "\n",
    "    normalize: true\n",
    "    num_epoch: 3\n",
    "    time_horizon: 1000\n",
    "    batch_size: 2024\n",
    "    buffer_size: 20240\n",
    "    gamma: 0.995\n",
    "    max_steps: 3e6\n",
    "    summary_freq: 3000\n",
    "    num_layers: 3\n",
    "    hidden_units: 512\n",
    "    \n",
    "The two most obvious changes are the ones in the two first graphs. <b>The agent definetly did learn</b> since the reward got higher with each iteration and <b>the simulation lasted more because the agent learned to balance itself</b>.\n",
    "\n",
    "Moving onto the second row of graphs, we can clearly see how the agent policy loss decreases as <b>the agent finds a suitable way to move</b> and starts to train that way of moving. The value loss from the policy increases as the agent explores strategies but then also decreases when the agent starts to focus on a moveset.\n",
    "\n",
    "At this point we've all noticed the weird stuff that happens in the first 150k iterations. We can conlcude that the agent found a maximum in the function that it was calculating and decided to move on since it wasn't an absolute maximum (the best solution to the problem).\n",
    "\n",
    "On the last graphs we can appreciate how the agent explores less and normalizes its behaviour as it keeps learning and estimates a higher reward as the reward curve (1st graphics from the 1st row) starts to grow steadily.\n",
    "\n",
    "\n",
    "\n",
    "### Second Training\n",
    "\n",
    "After learning from the mistakes we made in the first training, we decided to include the following variations:\n",
    "    - Custom agent rewards (stated in the previous Behaviour header)\n",
    "    - The target also resets when the agent fails.\n",
    "    \n",
    "This training was set to <b>3 million iterations</b> which took a whole night of calculations in order to complete. It was also run on the same configuration file as the previous one.\n",
    "\n",
    "<img width='900px' align='center' src='img/night-training.gif'>\n",
    "<center><cite>It ended up walking sideways</cite></center>\n",
    "\n",
    "After the changes, the tensorflow graphs showed us the struggles the agent had to learn a proper strategy.\n",
    "\n",
    "<img width='700px' align='center' src='img/night-training-graph-0.png'>\n",
    "<img width='700px' align='center' src='img/night-training-graph-1.png'>\n",
    "\n",
    "As we can see, <b>the reward graph is an absolute mess but it keeps growing between the spikes</b>. Note that the graph is smoothed a 92% so it can be read properly. We can also see that the time it took for the agent to end a simulation step did also have huge variations even in the 3 million steps, the agent has tried lots of ways to balance itself, this is why those graphs don't have a point in which they start to stabilize.\n",
    "\n",
    "In the second part we can see a similar result to the first simulation. The agent did find a way of walking that suited him and started mastering it like in the previous simulation. <b>This time though the choice didn't bring stabilized values al along it</b>.\n",
    "\n",
    "The last graphs also show how the agent starts exploring and experimenting but ends up sticking with the walking technique that fits him the best dues to the entropy and learning rate decrease. As those values decrease so does the estimated value, which starts to flatten.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "<b>The modifications we made to the agent were heavy</b> thus leaving us with an agent that needs a lot of iterations and some constraints apart from the joint movement restriction in order to work correctly. We're proud that we've made him jump and walk (even if is sideways) and achieve the target sometimes.\n",
    "\n",
    "We are aware that there are solvers which are way more efficient for the walker situation. Our testing did reinforce the common knowledge that robots have a hard time balancing themselves when moving on two legs.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
