{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the PushBlock case\n",
    "___\n",
    "## 1. Introduction\n",
    "In this notebook we analyse one of the examples in the ml-agents repository, the PushBlock example. \n",
    "Also we will analyse its performance with several parameter changes. And in the end we will take a look at a slightly modified version of the PushBlock case and see how it works compared to the original one.\n",
    "\n",
    "The team responsible for this work is formed by:  \n",
    "\n",
    "<img src=\"adria_foto.jpg\" style=\"width: 200px; margin: 10px 0px;  border: 1px solid black;\"> \n",
    "Adrià Ortiz Navarro  \n",
    "veric00@gmail.com  \n",
    "<img src=\"ferran_foto.jpg\" style=\"width: 200px; margin: 10px 0px;  border: 1px solid black;\"> \n",
    "Ferran Illa Capellas  \n",
    "ferran.illa26@gmail.com\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Case analysis\n",
    "The PushBlock example consists of a small block (the Agent) who has to push a bigger block (the Target) to the goal zone. This occur in a considerably small square platform with borders around it, so both the Agent and the Target can not fall from the platform.\n",
    "\n",
    "![pushblock](PushBlock_ss.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the **rewards** given to the Agent in the PushBlock case are the following:\n",
    "\n",
    "- +5 for reaching the Goal  \n",
    "This is the main reward given when the Agent completes his task.\n",
    "\n",
    "- -(1/`maxSteps`) for every frame  \n",
    "This negative reward is to encourage the Agent to complete his task as fast as possible, as the total reward decreases every frame. A negative reward like this will be necessary in any variation of the PushBlock case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for the brain to calculate the next Agent action, it needs to receive the **state** of the Agent inside the world. The information that the brain needs in the PushBlock example is the following:\n",
    "\n",
    "- Distance to Target\n",
    "- Distance to Goal\n",
    "- Distance to Walls\n",
    "\n",
    "It uses raycasts from the Agent to look for the objects mentioned above.\n",
    "```css\n",
    "var rayDistance = 12f;\n",
    "float[] rayAngles = { 0f, 45f, 90f, 135f, 180f, 110f, 70f };\n",
    "var detectableObjects = new[] { \"block\", \"goal\", \"wall\" };\n",
    "AddVectorObs(rayPer.Perceive(rayDistance, rayAngles, detectableObjects, 0f, 0f));\n",
    "AddVectorObs(rayPer.Perceive(rayDistance, rayAngles, detectableObjects, 1.5f, 0f));\n",
    "```\n",
    "What the Perceive function basically does is that, for each ray, stores categorial information on a detected object along with the object distance. This information is sent to the brain through the `AddVectorObs` method.\n",
    "\n",
    "<img src=\"raycasts.png\" style=\"width: 700px;\"> \n",
    "&nbsp;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important aspect of the process are the **actions**. These are the decisions of the brain. In the PushBlock case we have a discrete action space with only 6 actions posible. These actions correspond to 4 directions of movement (forward, back, right and left) and 2 directions of rotation (right and left). \n",
    "\n",
    "Depending on the action received, the right value is stored in the `rotateDir` and `dirToGo` variables, and then they are used to move the Agent in the scene.\n",
    "\n",
    "```css\n",
    "transform.Rotate(rotateDir, Time.fixedDeltaTime * 200f);\n",
    "agentRB.AddForce(dirToGo * academy.agentRunSpeed, ForceMode.VelocityChange);\n",
    "```\n",
    "<br><br>\n",
    "So summarizing, the learning process goes like this:\n",
    "\n",
    "The Agent does random actions and at some point he accidentally pushes the Target to the Goal zone, so he gets a high reward. After that, whenever the state of the Agent is similar to the one where he got a high reward, the Agent will also do similar actions to that step. Over time this situation will be repeated and the policy will be shaped. Also, the Agent will slowly learn to complete his task faster because of the negative reward over time of which we have spoken before. If the Agent pushes the Target to the Goal fast the reward will be higher so he will repeat those actions in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performance analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. New case proposal\n",
    "\n",
    "How to train:\n",
    "1. First, duplicate a brain, rename it if needed, and put the model to none \n",
    "2. Put the brain in the academy and check the control box\n",
    "3. Put the same brain into the GameObject Agent, inside PushAgentBasic, the part with Brain.\n",
    "4. In the Anaconda Prompt, put activate ml-agents and press Enter\n",
    "5. In the Anaconda Prompt put the direction of the folder ml-agents \n",
    "6. Once you are inside that folder, put mlagents-learn config/trainer_config.yaml --run-id=”NameOfTheLearning” --train\n",
    "7. Press enter, wait a few seconds, and then, press the play button. With this, the agent will train and learn\n",
    "\n",
    "If you like to train from a launcher and not from the editor:\n",
    "6. Create a .exe of the project\n",
    "7. Once you are inside that folder, put mlagents-learn config/trainer_config.yaml --env=FolderOfTheExecutable/Executable --run-id=”NameOfTheLearning” --train\t\n",
    "\n",
    "Rewards:\n",
    "- The target arrives to the goal = +5.0f\n",
    "- Every frame that the target isn’t in the goal = (-1f / agentParameters.maxStep)\n",
    "- If the agent is at a max distance of 3.0f with the target, every frame = + 0.001f\n",
    "- If the target is at a max distance of 1.0f with a wall, every frame = -0.01f\n",
    "\n",
    "States:\n",
    "- Only has the default state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"newCase_ss.png\" style=\"width: 700px;\">\n",
    "\n",
    "In this map, there are a few walls inside the path. Two are vertical and they aren’t a big problem, but the other cuts the map in 2 parts, putting difficulty to put the target into the goal. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
