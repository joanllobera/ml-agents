{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML-Agents delivey1 Soccer\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook contains all the information to understand how our example is implemented, how we tested it and the new case that we came up with. The example we took is the <b>Soccer</b> one.\n",
    "\n",
    "### Team members\n",
    "\n",
    "Name: David López Saludes.\n",
    "\n",
    "Email: davidlopezsaludes@enti.cat.\n",
    "<img src='img/David.png'>\n",
    "\n",
    "Name: Sergi Villalobos Gascón. \n",
    "\n",
    "Email: sergivillalobosgascon@enti.cat.\n",
    "<img src='img/Sergi.jpeg'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case analysis\n",
    "\n",
    "<b>a)</b> In the case of our example the rewards are 4 float variables being two of them the rewards and the other two the punishments. There are 4 variables because there are two types of brains that we want to train, the <b>Goalie</b> that has the role of defending the scoring area and the <b>Striker</b> who has the role of scoring, thus we have the variables called <kbd>strikerPunish</kbd> which gives a negative reward (-1) and <kbd>strikerReward</kbd> which gives a positive reward (+1) to the <b>Striker</b>. The same goes for the variables of the <b>Goalie</b> which are called <kbd>goaliePunish</kbd> and <kbd>goalieReward</kbd>, the first one works like the <b>Striker</b> ones but the second one returns 0 so there is no reward for the goalie unless we modify it on the inspector.\n",
    "\n",
    "<img src='img/RewardsandPunishments.jpg'>\n",
    "\n",
    "This variables can be changed on the Unity inspector, these are the default values:\n",
    "\n",
    "<img src='img/RewardsandPunishmentsOnInspector.jpg'>\n",
    "\n",
    "This variables are on the <kbd>SoccerAcademy</kbd> script.\n",
    "\n",
    "<b>b)</b> On this example the states will be represented as a class named <kbd>PlayerState</kbd> which embodies the characteristics of the player, like the <kbd>playerIndex</kbd> which will be recived from the <kbd>AgentSoccer</kbd> script; the <kbd>agentRB</kbd> which is the rigid body of the agent, the <kbd>AgentSoccer</kbd> type called <kbd>agentScript</kbd> that will be used to know the characteristics of the agent like which team it is into or if it is a striker or a goalie, and the variable <kbd>ballPosReward</kbd>.\n",
    "\n",
    "<img src='img/PlayerState.jpg'>\n",
    "\n",
    "This class will be used for the four agents that will be generated on the example and it will be saved in a list called <kbd>PlayerStates</kbd> which will save all the <kbd>PlayerState</kbd> classes to be used to assign the rewards and punishments so the brain can be trained.\n",
    "\n",
    "<img src='img/PlayerStateList.jpg'>\n",
    "\n",
    "This elements can be found on the script called <kbd>SoccerFieldArea</kbd>.\n",
    "\n",
    "<b>c)</b> The training is implemented using two diferent kind of brains the brain for the <b>Goalie</b> which we called <kbd>NewGoalieLearning</kbd> and the brain for the <b>Striker</b> which is called <kbd>NewStrikerLearning</kbd>, each of this brains will be attached to the script that corresponds to the class that they should be on. \n",
    "\n",
    "<img src='img/Brains.jpg'>\n",
    "\n",
    "After implementig this brains we will input the necessary parameters to the <b>Academy</b> game object in order to proceed with the training, speciallt attaching the brains to this game object script and set them on control mode.\n",
    "\n",
    "<img src='img/AcademyScript.jpg'>\n",
    "\n",
    "Note that we can change the number of observations our agent makes on the iteration by changing the space size on the <kbd>vector observation</kbd> on the brain inspector.\n",
    "\n",
    "<img src='img/BrainInspector.jpg'>\n",
    "\n",
    "After this is done we have to set the parameters on the <kbd>trainer_config.yaml</kbd> to the values we want so the training proceeds as planed, the folder where is placed the <kbd>.yaml</kbd> is called config, it must be open via notepad. On this file we can change the number of steps that we want our example to train, the speed of the stepts it takes, etc.\n",
    "\n",
    "<img src='img/trainerConfigFile.jpg'>\n",
    "\n",
    "After that the command <kbd>mlagents-learn config/trainer_config.yaml --run-id=X --train</kbd> (if wanted to be trained on build this is the command <kbd>mlagents-learn config/trainer_config.yaml --env=../../projects/Cats/CatsOnBicycles.app --run-id=cob_1 --train</kbd>) must be entered on the <b>anaconda prompt</b> in orther for the training to start. With this done we have to revise the code on the unity scripts.\n",
    "\n",
    "<img src='img/anacondaPromptCommand.jpg'>\n",
    "\n",
    "The training method is implemented on four scripts, this are called <kbd>AgentSoccer</kbd>, <kbd>SoccerAcademy</kbd>, <kbd>SoccerBallController</kbd> and <kbd>SoccerFieldArea</kbd>. \n",
    "\n",
    "The script <kbd>AgentSoccer</kbd> keeps the information of the agent, having here the information that refers to the agent's team, the actions that it will take and the roles that will adopt. \n",
    "\n",
    "The script <kbd>SoccerAcademy</kbd> will act as the manager of the training, initializing the important variables like the punishments, the rewards, the brains, the agent steps, etc. \n",
    "\n",
    "<img src='img/VariablesOnAcademy.jpg'>\n",
    "\n",
    "The script <kbd>SoccerBallController</kbd> will act as a manager of the ball checking if it has interacted with the score line or not and which team has scored.\n",
    "\n",
    "<img src='img/SoccerBallController.jpg'>\n",
    "\n",
    "The script <kbd>SoccerFieldArea</kbd> is the most important of all, being the one that embodies the main functions that will make the training possible. The functions that will make that possible are the following: <kbd>GoalTouched</kbd> and <kbd>RewardOrPunishPlayer</kbd>. To use correctly this functions on the script we create a class, which is the same class that we mentioned on the <b>b)</b> section, which is called <kbd>PlayerState</kbd>, this will define the agent characteristics and then it will be added to the the list <kbd>playerStates</kbd> which will contain all the agents that will be trained. After having this clear we go to the function <kbd>GoalTouched</kbd>, that will iterate on the list that we mentioned and check which team has scored and give them a reward for it, or a punish in the case they got scored, it will also restart the coroutine after the scoring and also will randomize the players teams for training.\n",
    "\n",
    "<img src='img/GoalTouched.jpg'>\n",
    "\n",
    "Then we have the <kbd>RewardOrPunishPlayer</kbd> function that will be used on the function we have just seen, it will recive three parameters: the player, the striker rewards/punishes and the goalie rewards/punishes; it will activate when a team has scored (as we have seen on the previous paragraph) and it will give a reward or a punish to the striker and goalie depending if they where scored or they scored. \n",
    "\n",
    "<img src='img/RewardOrPunishPlayer.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance analysis\n",
    "\n",
    "We have trained the agents 8 times the first 4 were trained on the Unity editor with the brains that we created to train (<kbd>NewGoalieLearning</kbd> and <kbd>NewStrikerLearning</kbd>), and the last 4 were trained using the new brains on the blue team and the unity trained (<kbd>GoalieLearning</kbd> and <kbd>StrikerLearning</kbd>) brains on the red team, this last one was tested on a .exe.\n",
    "\n",
    "<img src='img/Brains.jpg'>\n",
    "\n",
    "On the first four cases we trained 2 brains only with 50000 steps and the las two were trained with 500000 steps, the last four brains were trained more randomly, being the fits a 50000 steps training, the second 500000 steps training, the tird 499000 steps training and the last one 50000 to make clear some doubts we had. After all this steps we got to the conclution that this is one of the most difficult examples to train due to the unclear reward-punishment system, because we didn't change the variables that where given by default and the brains were unable to learn correctly until we made them train against the already ml-agents default trained brain.\n",
    "\n",
    "To see the results of the training we have chosen three examples on <b>TensorBoard</b> to show the training, the first will be the 50000 steps (doesn't matter if it's trained on exe or in unity), the second will be a example trained with 500000 steps on the unity editor and the third will be a 500000 steps trained on a executable field against the ml-agents trained brain.\n",
    "\n",
    "### Case 1\n",
    "\n",
    "<img src='img/TensorBoard1.jpg'>\n",
    "\n",
    "On the example we can see that there are two lines, the blue one refers to the <b>Goalie</b> and the pink one to the <b>Striker</b>, there are some breaks on the line that go straight to the beginning, that is because sometimes the computer crashed and we had to restart the training. First we start with the <kbd>Environment/Cumulative Reward</kbd> wich shows the mean cumulative episode reward over all agents, and it should increase in the case of a successful training session; but as we can see on the board in the case of the <b>Goalie</b> there is a increasement but the <b>Striker</b> has a decreasement, we could see that when executed the brain file the <b>Striker</b> had no idea where to score. Then on the <kbd>Policy/Entropy</kbd> board we can see that the randomnes of the choices of the agents greatly decreases on the execution so we understood that it was learning something but not the right thing, we can also see that the <kbd>Policy/Learning Rate</kbd> works correctly because it decreases over time but when we look atthe <kbd>Policy/Value Estimate</kbd> in the case of the <b>Goalie</b> it ascends, not as it should but it does; but in the other hand the <b>Striker</b> stays the same all the time, that shows us that there is always a mean value and it never decreases. With that on mind we can only look at the <b>Learning Loss Functions</b> to see what they have to say, on the <kbd>Losses/Policy Loss</kbd> we can see that in the case of the <b>Goalie</b> it decreases slightly showing that there is not too much of a change on the ploicy, and then on the <b>Striker</b> we see that it slightly increases showing that it changes less than the <b>Goalie</b>. And last but not least we check the <kbd>Losses/Value Loss</kbd> seeing that in both cases it stays almost the same all the time so it is clear that is no predictment of the value on each state. \n",
    "\n",
    "On this example we could see that for some reason the brain is not learning properly what gives the brain 0 capacity for evolving and learning something. \n",
    "\n",
    "### Case 2\n",
    "\n",
    "<img src='img/TensorBoard2.jpg'>\n",
    "\n",
    "On this case the <b>Goalie</b> is represented by the orange line and the <b>Striker</b> is represented by the blue one. Here we checked the <kbd>Environment/Cumulative Reward</kbd> we see again an increasement on the  <b>Goalie</b> that we can't see on the <b>Striker</b>, meaning that the <b>Striker</b> still did not learn. This time on the <kbd>Policy/Entropy</kbd> there was again a notable decreasement on the board showing that there was an actual randomness on the choices, and the same goes for the <kbd>Policy/Learning Rate</kbd> wich is exactly the same than the last example and for the <kbd>Policy/Value Estimate</kbd> with the difference that in this case the <b>Striker</b> and the <b>Goalie</b> do the opposite. Then we look at the <kbd>Losses/Policy Loss</kbd> board it goes worst than in the last example because this time it doesn't increase or decrease meaning that the policy doesn't change, and <kbd>Losses/Value Loss</kbd> increases all the time, meaning that there is no reward stabilization.\n",
    "\n",
    "This example gives us the same result as the last one, wich means that giving the brain a larger time span for training is not what will make it learn.\n",
    "\n",
    "### Case 3\n",
    "\n",
    "<img src='img/TensorBoard3.jpg'>\n",
    "\n",
    "On this case the <b>Goalie</b> is represented by the blue line and the <b>Striker</b> is represented by the brown line. On this case we have trained our brain against the <b>ml-agents</b> trained brain and is executed on a .exe file. First we check the <kbd>Environment/Cumulative Reward</kbd> we have the same situation of the last two examples, <kbd>Policy/Entropy</kbd> and <kbd>Policy/Learning Rate</kbd> decrease as before but in the <kbd>Policy/Value Estimate</kbd> the <b>Goalie</b> increases notably meaning that it is training correctly, not like the <b>Striker</b> that stays the same all the time. In this case the <kbd>Losses/Policy Loss</kbd> changes drastically all the time showing that the policy is changing constantly, it can be because it is being trained against an already trained agent making it more difficult to adapt. Refering to the <kbd>Losses/Value Loss</kbd> the <b>Goalie</b> part succeeds on it increasing at the beginning and decreasing and\n",
    "stabilizing towards the end, cannot say the same of the <b>Striker</b> wich stays the same all the time showing that there is no prediction and no learning.\n",
    "\n",
    "On this last example we saw that the <b>Goalie</b> has learned but the <b>Striker</b> has not, so we conclude that the right way to train this example is by making it train against an already trained brain, at least for the <b>Goalie</b>.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New case proposal\n",
    "\n",
    "Regarding the difficulty of training this example due that the rewards and punishments are not given correctly we thought on creating a better example just by adding a brain for each team and role instead of making just de distintion of the role, but it will not be the case of what we really want to implement due to it's easy achivement, so we came up with the idea of creating an example where the agent could not shoot the ball outside the field.\n",
    "\n",
    "To create the example we'll have to add on the <kbd>SoccerBallController</kbd> script a variable that would check if the ball collided with the wall, that variable could be named <kbd>wallTag</kbd>. Then this variable will go on the fucntion <kbd>OnCollisionEnter</kbd> as a condition to activate a function called <kbd>WallTouched</kbd> that would give the punishment for touching the wall.\n",
    "\n",
    "<img src='img/NewSoccerBallController.jpg'>\n",
    "\n",
    "The function <kbd>WallTouched</kbd> would work the same way as the function <kbd>GoalTouched</kbd> with the difference that it will only give a punishment and no rewards because the objective of this training method is to make sure that the players don't shoot the ball out of the field. Here we can see how the new function would look.\n",
    "\n",
    "<img src='img/WallTouched.jpg'>\n",
    "\n",
    "In addition to this, the best option for making the training smoother would be to expand the field to give more space to the agents to shoot the ball and train better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
